{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "from gensim.models import KeyedVectors\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(documents, dim=300, min_count=2, iters=100, window=5, negative=5):\n",
    "\tmodel = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        sg=1,\n",
    "        size=dim,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "#         max_final_vocab=3000,\n",
    "        sample=1e-5,\n",
    "        iter=iters,\n",
    "        ns_exponent=0.75,\n",
    "        negative=negative,\n",
    "        workers=4)\n",
    "\tmodel.train(documents, total_examples=len(documents), epochs=model.epochs)\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(csv_file, column='stem'):\n",
    "    \"\"\"read stem utterances from childes csv files\"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    tags = df['part_of_speech'].values\n",
    "    stems = df['stem'].values\n",
    "    ret_list = []\n",
    "    for t, s in zip(tags, stems):\n",
    "        tl, sl = str(t).lower().split(), str(s).lower().split()\n",
    "        \n",
    "        # replace NAME and interjections with $name$ and $co$ respectively\n",
    "        ntl = []\n",
    "        for t, s in zip(tl, sl):\n",
    "            if t == \"n:prop\":\n",
    "                ntl.append('$name$')\n",
    "#             elif t == 'co':\n",
    "#                 ntl.append('$co$')\n",
    "            else:\n",
    "                ntl.append(s)\n",
    "\n",
    "#         print(' '.join(ntl))\n",
    "        ret_list.append(ntl)\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/change/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3242: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/change/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3242: DtypeWarning: Columns (11,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 675, 0, 2996, 15663, 17582, 25704, 43371, 2235, 18595, 73200, 93436, 109389, 62921, 80428, 93764, 194213, 153429, 161326, 189616, 222385, 281451, 574601, 499293, 515099, 532281, 505978, 548514, 670957, 565947, 469748, 501356, 464451, 405575, 776889, 313177, 205337, 230318, 154513, 155255, 259921, 169736, 139363, 188878, 154077, 146393, 195701, 127483, 113799, 109585, 124600, 97402, 198899, 129798, 88846, 660399, 76209, 94505, 65465, 46699, 40163, 34145, 37276, 33480, 84709, 67468, 34144, 40616, 27063, 28027, 15985, 17189, 14914, 14057, 9497, 9951, 9696, 16858, 12940, 16609, 6876, 30265, 26949, 38818, 20143, 18162, 19409, 16892, 22755, 29847, 10955, 6206, 9887, 13402, 1879, 2702, 3110, 5285, 2891, 3035, 4418, 2771, 4252, 340, 6896, 6225, 2122, 7346, 4187, 5699, 11342, 6588, 10035, 8916, 12356, 11844, 1925, 4827, 14867, 6036, 2715, 5008, 154, 1968, 2625, 39, 28, 27, 493, 74, 2514, 9475, 19, 6, 23, 151, 3801, 24, 2093, 681, 2, 11, 667, 0, 5070, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "childes_files = sorted(glob.glob(\"./data/childes-en/*.csv\"))\n",
    "num_tokens = []\n",
    "for filename in sorted(childes_files, key=lambda x: int(x.split('_')[-1][:-4])):\n",
    "    month = int(filename.split('_')[-1][:-4])\n",
    "    lines = read_docs(filename)\n",
    "    num_tokens.append(sum([len(l) for l in lines]))\n",
    "print(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is taken from `1-train_word_embeddings-English` so that the shuffled corpus\n",
    "# have similar size as the original (time-series) corpus\n",
    "controlled_windows= [2416980, 2052651, 2255166, 2148271, 2116968, 2017226]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before shuffling:\n",
      "['just', 'like', 'your', 'book', 'at', 'home']\n",
      "After shuffling\n",
      "['a', 'bat']\n",
      "Training 0th period w/ 2416986 tokens:\n",
      "Finished training.\n",
      "Training 1th period w/ 2052655 tokens:\n",
      "Finished training.\n",
      "Training 2th period w/ 2255167 tokens:\n",
      "Finished training.\n",
      "Training 3th period w/ 2148278 tokens:\n",
      "Finished training.\n",
      "Training 4th period w/ 2116971 tokens:\n",
      "Finished training.\n",
      "Training 5th period w/ 2017228 tokens:\n",
      "Finished training.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "childes_files = sorted(glob.glob(\"./data/childes-en/*.csv\"), key=lambda x: int(x.split('_')[-1][:-4]))\n",
    "year2vecs = {}\n",
    "\n",
    "docs = []  \n",
    "for filename in childes_files:\n",
    "    docs.extend(read_docs(filename))\n",
    "    \n",
    "print(\"Before shuffling:\")\n",
    "print(docs[0])\n",
    "random.shuffle(docs)\n",
    "print(\"After shuffling\")\n",
    "print(docs[0])\n",
    "\n",
    "num_tokens = 0\n",
    "each_docs = []\n",
    "which_period = 0\n",
    "for doc in docs:\n",
    "    each_docs.append(doc)\n",
    "    num_tokens += len(doc)\n",
    "    \n",
    "# start and end index of docs\n",
    "start, end = 0, 0\n",
    "for i, window_size in enumerate(controlled_windows):\n",
    "    num_tokens = 0\n",
    "    docs_per_period = []\n",
    "    \n",
    "    for j, doc in enumerate(docs[start:]):\n",
    "        docs_per_period.append(doc)\n",
    "        num_tokens += len(doc)\n",
    "        if num_tokens > window_size:\n",
    "            print(f\"Training {i}th period w/ {num_tokens} tokens:\")\n",
    "            model = train_word2vec(docs_per_period, dim=100, min_count=15, iters=50, window=5)\n",
    "            d = {w:v for w, v in zip(model.wv.index2word, model.wv.vectors)}\n",
    "            year2vecs['period'+str(i)] = d\n",
    "            print(\"Finished training.\")\n",
    "            \n",
    "            # reset the params\n",
    "            num_tokens = 0\n",
    "            docs_per_period = []\n",
    "            start += j\n",
    "            break\n",
    "\n",
    "print(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# childes_files = sorted(glob.glob(\"./data/childes-en/*.csv\"), key=lambda x: int(x.split('_')[-1][:-4]))\n",
    "# year2vecs = {}\n",
    "# for i, period in enumerate(periods):\n",
    "#     print(np.array(childes_files)[period])\n",
    "#     print('key', 'period'+str(i))\n",
    "#     docs = []\n",
    "#     for filename in np.array(childes_files)[period]:\n",
    "#         docs.extend(read_docs(filename))\n",
    "#     model = train_word2vec(docs, dim=100, min_count=15, iters=50, window=5)\n",
    "#     d = {w:v for w, v in zip(model.wv.index2word, model.wv.vectors)}\n",
    "#     year2vecs['period'+str(i)] = d\n",
    "# print(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/embeddings-over-time/embeddings-English-1M-ep50-f15-shuffle.pickle', 'wb') as handle:\n",
    "    pickle.dump(year2vecs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('filename.pickle', 'rb') as handle:\n",
    "#     b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
